# -*- coding: utf-8 -*-
"""Spam_filtering_using_Glove_Embedding .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16_6G2yKE46o8PYsjBIkYkbxgK-wgjvR6
"""

import numpy as np
import pandas as pd
import re
import nltk
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

df = pd.read_csv('/content/spam.csv',encoding='latin-1')

df.head()

# converting to binary data
df_one = pd.get_dummies(df["v1"])
# print(df_one)
  
# display result
df_two = pd.concat((df_one, df), axis=1)
df_two = df_two.drop(["v1"], axis=1)
df_two = df_two.drop(["ham"], axis=1)
df1 = df_two.rename(columns={"spam": "v1"})
df1.head()

df1.drop("Unnamed: 3",inplace=True,axis=1)
df1.drop("Unnamed: 4",inplace=True,axis=1)
df1.drop("Unnamed: 2",inplace=True,axis=1)

df1.head()

"""<h3>Cleaning and Preparing text</h3>
<ul>
    <li>Cleaning links</li>
    <li>Cleaning digits except alphabetical and numeric characters</li>
    <li>Lowering</li>
    <li>Tokenising</li>
    <li>Lemmatizing and Removing Stop words</li>
    <li>Bag of words</li>
</ul>

<h3>Cleaning links</h3>
<p>There are links in the mail such as: <a>https://google.com.tr</a>. If we don't remove them they can cause problems.</p>
<p>To clean the following links, we can use Regex(regular) expression.A regular expression is a special sequence of characters that helps you match or find other strings or sets of strings, using a specialized syntax held in a pattern. Regular expressions are widely used in UNIX world. </p>
"""

x = df1["v2"]
x_clnd_link = [re.sub(r"http\S+", "", text) for text in x]

print(x_clnd_link[0])

"""<h3>Cleaning digits except alphabetical and numeric characters</h3>
<p>As you can see from the text above, there are a lot of digits such as <b>*</b> and <b>:</b> They don't have a meaning, so we should remove them from the texts.

In order to clean unrelevant digits we'll use regex again.</p>
"""

pattern = "[^a-zA-Z0-9]"

"""

<p>-This helps to remove special characters</p>"""

#This means to replace all the characters following the pattern
x_cleaned = [re.sub(pattern, " ",text) for text in x_clnd_link]

print(x_cleaned[0])

"""<p><b>Now let's lower the texts, I won't add a section for it because it is a familiar process from the vanilla python.

</b></p>
"""

x_lowered = [text.lower() for text in x_cleaned]

print(x_lowered[0])

"""<h3>Tokeninzing</h3>
<p>In order to create a feature that shows whether the text includes the word or not, we need to split words into lists, we can do this using pythonString.split() but there is a better function to do this in NLTK.

Let's tokenize the texts.</p>
"""

nltk.download('punkt')

x_tokenized = [nltk.word_tokenize(text) for text in x_lowered]

print(x_tokenized[0])

"""<p>- Each sentence turned into a list that contains words.</p>

<h3>Lemmatizing and Removing Stopwords</h3>
<p>In natural languages, words can get additional so each word can have a lot of versions, sometimes these additionals may give tips to us but in filtering spams, we don't need them.There are two ways to remove additionals: <b>Stemmers and Lemmatizers</b> </p>

<h3>Stemmers</h3>
<p>Stemmers are rule based weak tools, they remove additionals using rules but in natural languages everything does not follow the rules. Also It cant change tenses, for instance lemmatizers convert learnt into learn, stemmers don't touch them. Although stemmers are weak they are fast and although so many natural language do not have lemmatizers most of them have stemmers.</p>

<h3>Lemmatizers</h3>
<p>Lemmatizers uses dictionaries to remove additionals and change tenses. They work good but developing a lemmatizer is hard and needs a lot of resource, so they are rare. Also lemmatizers use dictionaries, and that causes lemmatizers being slow.

In this kernel we'll use NLTK's WordNet Lemmatizer. WordNet is a big dictionary.</p>
"""

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
lemma = WordNetLemmatizer()

"""<p>- Now, we can easily use lemmatizer</p>"""

words = ["pets","bats","cats","removed","beers","went","stopped","studied"]
for word in words:
    print(lemma.lemmatize(word), end=" ")

"""<p>- We can now lemmatize our tokenized text</p>"""

x_lemmatized = [[lemma.lemmatize(word) for word in text] for text in x_tokenized]

print(x_lemmatized[0])

"""<h3>Removing Stopwords</h3>
<p>In natural languages there are words that not have a special meaning such as <b>will</b>, it is always a tense and such as <b>and,or</b>

In order to win from time and improve the model we should remove them. There are several ways to remove them but in this kernel we'll use stopwords corpora of NLTK. There are stopwords of 11 natural language in there.</p>
"""

import nltk
nltk.download('stopwords')

stopwords = nltk.corpus.stopwords.words('english')
x_prepared = [[word for word in text if word not in stopwords] for text in x_lemmatized]

print(x_prepared[0])

"""<p>Let's see the unique words of our dataset</p>

<h3>Bag of words</h3>
<p>And we came to the final process of this section: Bag of Words. Bag of Words is an easy approach to make sense of texts.</p>
"""

vectorizer = CountVectorizer(max_features=20000)
x = vectorizer.fit_transform([" ".join(text) for text in x_prepared]).toarray()

x.shape

x_train,x_test,y_train,y_test = train_test_split(x_prepared,np.asarray(df1["v1"]),random_state=42,test_size=0.2)

y_train

"""# Play around with Word2vector"""

!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove.6B.zip

!python -m gensim.scripts.glove2word2vec -i glove.6B.300d.txt -o glove.6B.300d.word2vec.txt

import nltk
nltk.download('punkt')
nltk.download('wordnet')

from gensim.models import KeyedVectors

w2v = KeyedVectors.load_word2vec_format('glove.6B.300d.word2vec.txt',binary=False)

w2v.most_similar('king')

# King + Woman - Man = ?
w2v.most_similar(['king','woman'],negative=['man'],topn=1)

w2v.doesnt_match("england china vietnam laos".split())

"""## Use Glove Pretrain"""

import matplotlib.pyplot as plt
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout,GRU, Activation ,Conv1D
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers

maxlen = 2000
max_features = 7098

EMBEDDING_FILE = 'glove.6B.300d.txt'
tokenizer = Tokenizer(num_words=max_features)

tokenizer.fit_on_texts(x_train)

x_train_features = np.array(tokenizer.texts_to_sequences(x_train))
x_test_features = np.array(tokenizer.texts_to_sequences(x_test))

x_train_features = pad_sequences(x_train_features,maxlen=maxlen)
x_test_features = pad_sequences(x_test_features,maxlen=maxlen)

"""# Neural Network Embedding From Scratch"""

from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,GRU
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

embed_size = 100 # how big is each word vector
max_feature = 50000 # how many unique words to use (i.e num rows in embedding vector)
max_len = 2000 # max number of words in a question to use

tokenizer = Tokenizer(num_words=max_feature)

tokenizer.fit_on_texts(x_train)

x_train_features = np.array(tokenizer.texts_to_sequences(x_train))
x_test_features = np.array(tokenizer.texts_to_sequences(x_test))

x_train_features = pad_sequences(x_train_features,maxlen=max_len)
x_test_features = pad_sequences(x_test_features,maxlen=max_len)

embed_size = 100

inp = Input(shape=(max_len,))
x = Embedding(max_feature, embed_size)(inp)
x = Bidirectional(GRU(64, return_sequences=True))(x)
x = GlobalMaxPool1D()(x)
x = Dense(16, activation="relu")(x)
x = Dropout(0.1)(x)
x = Dense(1, activation="sigmoid")(x)
model = Model(inputs=inp, outputs=x)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

print(model.summary())

history = model.fit(x_train_features, y_train, batch_size=512, epochs=10, validation_data=(x_test_features, y_test))

plt.plot(history.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Neural Network"""

from sklearn.metrics import confusion_matrix,f1_score, precision_score,recall_score

y_predict  = [1 if o>0.5 else 0 for o in model.predict(x_test_features)]

confusion_matrix(y_test,y_predict)

tn, fp, fn, tp = confusion_matrix(y_test,y_predict).ravel()

print("Precision: {:.2f}%".format(100 * precision_score(y_test, y_predict)))
print("Recall: {:.2f}%".format(100 * recall_score(y_test, y_predict)))

f1_score(y_test,y_predict)

"""## Error Analysis

### Plot confusion matrix
"""

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import itertools

cnf_matrix = confusion_matrix(y_test,y_predict)

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Reds):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

"""# Plot normalized confusion matrix"""

plt.figure()
plot_confusion_matrix(cnf_matrix, classes=['Non Spam','Spam'], normalize=False,
                      title='Confusion matrix')

from sklearn.metrics import auc
from sklearn.metrics import roc_curve
fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_predict)
auc_keras = auc(fpr_keras, tpr_keras)
plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve')
plt.legend(loc='best')
plt.show()

